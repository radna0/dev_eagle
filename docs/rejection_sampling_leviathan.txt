JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Fast Gumbel-Max Sketch and its Applications

arXiv:2302.05176v1 [cs.LG] 10 Feb 2023

Yuanming Zhang, Pinghui Wang, Yiyan Qi, Kuankuan Cheng, Junzhou Zhao,
Guangjian Tian and Xiaohong Guan
Abstract—The well-known Gumbel-Max Trick for sampling elements from a categorical distribution (or more generally a non-negative
vector) and its variants have been widely used in areas such as machine learning and information retrieval. To sample a random element
i in proportion to its positive weight vi , the Gumbel-Max Trick first computes a Gumbel random variable gi for each positive weight
element i, and then samples the element i with the largest value of gi + ln vi . Recently, applications including similarity estimation and
weighted cardinality estimation require to generate k independent Gumbel-Max variables from high dimensional vectors. However, it is
computationally expensive for a large k (e.g., hundreds or even thousands) when using the traditional Gumbel-Max Trick. To solve this
problem, we propose a novel algorithm, FastGM, which reduces the time complexity from O(kn+ ) to O(k ln k + n+ ), where n+ is the
number of positive elements in the vector of interest. FastGM stops the procedure of Gumbel random variables computing for many
elements, especially for those with small weights. We perform experiments on a variety of real-world datasets and the experimental
results demonstrate that FastGM is orders of magnitude faster than state-of-the-art methods without sacrificing accuracy or incurring
additional expenses.
Index Terms—Gumbel-Max Trick, Sketching, Jaccard Similarity Estimation, Weighted Cardinality Estimation

F

1

I NTRODUCTION

The Gumbel-Max Trick [2] is a popular technique for
sampling elements from a categorical distribution (or more
generally a non-negative vector), which has been widely used
in many areas. Given a non-negative vector ~v = (v1 , . . . , vn ),
let N~v+ , {i : vi > 0, i = 1, . . . , n} be the set of indices
of positive elements in ~v . Then, the Gumbel-Max Trick
computes a random variable s(~v ) as:

s(~v ) , argmax − ln(− ln ai ) + ln vi ,
+
i∈N~
v

where ai is a random variable drawn from the uniform distribution UNI(0, 1) independently and so gi = − ln(− ln ai )
is a Gumbel random variable. Note that different vectors ~v
should use the same set of variables a1 , . . . , an to guarantee consistency. The distribution of random variable s(~v )
is P (s(~v ) = i) = Pnvi vj . Therefore, the Gumbel-Max
j=1
Trick is popularly applied to sample an element from a
•

•

•

•

An earlier conference version of this paper appeared at the Proceedings of
The Web Conference 2020 [1]. In this extended version, we extend the
Gumbel-Max sketch’s definition, which can be applied to more applications.
Compared with the conference version, we also propose a more efficient
method FastGM to compute the Gumbel-Max sketch and include new
experiments for the application of weighted cardinality estimation.
Y. Zhang, P. Wang, Y. Qi, K. Cheng, and J. Zhao are with the
MOE Key Laboratory for Intelligent Networks and Network Security, Xi’an Jiaotong University, P.O. Box 1088, No. 28, Xianning West Road, Xi’an, Shaanxi 710049, China. E-mail: {zhangyuanming, kuankuan.cheng}@stu.xjtu.edu.cn, phwang@mail.xjtu.edu.cn,
qiyiyan@idea.edu.cn, junzhou.zhao@xjtu.edu.cn.
X. Guan is with the MOE Key Laboratory for Intelligent Networks and
Network Security, Xi’an Jiaotong University, P.O. Box 1088, No. 28,
Xianning West Road, Xi’an, Shaanxi 710049, China and also with the
Center for Intelligent and Networked Systems, Tsinghua National Lab for
Information Science and Technology, Tsinghua University, Beijing 100084,
China. E-mail: xhguan@mail.xjtu.edu.cn.
G. Tian is with Huawei Noah’s Ark Lab, Hong Kong. E-mail:
Tian.Guangjian@huawei.com.

Corresponding author: Pinghui Wang
Manuscript received April 19, 2005; revised August 26, 2015.

high-dimensional non-negative vector ~v with probability
proportional to the element’s weight.
We call s(~v ) and x(~v ) = maxi∈N + − ln(− ln ai ) + ln vi
~
v
as Gumbel-ArgMax and Gumbel-Max variables of vector
~v , respectively. In this paper, we define a Gumbel-Max
sketch of vector ~v as a vector of k Gumbel-Max variables
generated independently, i.e., ~
x(~v ) = (x1 (~v ), . . . , xk (~v )),
where xj (~v ) = maxi∈N + − ln(− ln ai,j ) + ln vi , j = 1, . . . , k
~
v
and ai,j ∼ UNI(0, 1). Similarly, we define a Gumbel-ArgMax
sketch of vector ~v as a vector of k Gumbel-ArgMax variables
generated independently, i.e., ~s(~v ) = (s1 (~v ), . . . , sk (~v )). For
simplicity, we also name the Gumbel-ArgMax sketch as the
Gumbel-Max sketch when no confusion arises. We observe
that the Gumbel-Max sketch has been actually exploited
for applications including probability Jaccard similarity
estimation [3], [4], [5], [6], [7] and weighted cardinality
estimation [8], while the authors of these works might be
unconscious of this.
Probability Jaccard Similarity Estimation. Similarity
estimation lies at the core of many data mining and machine
learning applications, such as web duplicate detection [9],
[10], collaborate filtering [11] and association rule learning
[12]. To efficiently estimate the similarity between two
vectors, several algorithms [3], [4], [5], [6] compute k random
ln a
ln a
variables − vii,1 , . . . , − vii,k for each positive element vi
in ~v , where ai,1 , . . . , ai,k are independent random variables
drawn from the uniform distribution UNI(0, 1). Then, these
algorithms build a sketch of vector ~v consisting of k registers,
and each register records sj (~v ) where

sj (~v ) = argmin −
+
i∈N~
v

ln ai,j
,
vi

1 ≤ j ≤ k.

(1)

We find that sj (~v ) is exactly a Gumbel-ArgMax variable
ln a
of vector ~v as argmini∈N + − vii,j = argmaxi∈N + ln vi −
~
v
~
v
ln(− ln ai,j ). Let 1(x) be an indicator function. Yang et al. [3],
P
k
[4], [5] use k1 j=1 1(sj (~
u) = sj (~v )) to estimate the weighted

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

2

Jaccard similarity of two non-negative vectors ~
u and ~v which
is defined by
Pn
min{ui , vi }
JW (~u, ~v ) , Pni=1
.
max{u
i , vi }
i=1

Recently, Moulton et al. [6] prove that the expectation of esPk
timate k1 1 1(sj (~
u) = sj (~v )) actually equals the probability
Jaccard similarity, which is defined by
X
1

.
JP (~u, ~v ) ,
Pn
u l vl
l=1 max ui , vi
i∈N +
~
v ,~
u

Here, N~v+,~u , {i : vi > 0 ∧ ui > 0, i = 1, . . . , n} is the set

of indices of positive elements in both ~v and ~
u. Compared
with the weighted Jaccard similarity JW , Moulton et al.
demonstrate that the probability Jaccard similarity JP is
scale-invariant and more sensitive to changes in vectors.
Moreover, each function sj (~v ) maps similar vectors to the
same value with a high probability. Therefore, similar to
regular locality-sensitive hashing (LSH) schemes [13], [14],
[15], one can use these Gumbel-Max sketches to build an
LSH index for fast similarity search in a large dataset, which
is capable to search similar vectors for any query vector in
sub-linear time.
Weighted Cardinality Estimation. Given a sequence Π =
o1 o2 · · · , where oj ∈ {1, . . . , n} represents an object (e.g., a
string) and each object i ∈ {1, . . . , n} may appear more
than once. Each object i has a positive weight vi . Let NΠ
be the set of objects that occurred in P
Π. Then, the weighted
cardinality of Π is defined as cΠ = i∈NΠ vi . Take a SQL
query "SELECT DISTINCT CompanyNames FROM Orders"
as an instance. The size (in bytes) of the query result is a
sum weighted by string length over the "CompanyNames".
Besides the cardinality of a single sequence, sometimes, the
data of interest consists of multiple sequences distributed
over different locations and the target is to estimate the
sum of all unique occurred objects’ weights using as few
resources (including memory space, computation time, and
communication cost) as possible. The state-of-the-art method
of weighted cardinality estimation is Lemiesz’s sketch [8].
Let ~v Π = (v1Π , . . . , vnΠ ) be the underlying vector of sequence
Π. That is, each element viΠ , i = 1, . . . , n equals vi (i.e.,
the weight of object i) when object i occurs in sequence Π
(i.e., i ∈ NΠ ) and 0 otherwise. Lemiesz computes a sketch
~y (~v Π ) = (y1 (~v Π ), . . . , yk (~v Π )), and yj (~v Π ) is defined as:

yj (~v Π ) = min −
i∈NΠ

ln ai,j
,
vi

1 ≤ j ≤ k,

(2)

where all variables ai,j ∼ UNI(0, 1) are independent with
each other. Then, we easily find that the Gumbel-Max variable xj (~v Π ) = maxi∈NΠ − ln(− ln ai,j ) + ln vi = − ln yj (~v Π ).
Therefore, Lemiesz’s sketch is a variant of the GumbelMax sketch. It is easy to find that each yj (~v Π ) follows
the exponential distribution EXP(cΠ ) because P (yj (~v Π ) ≥
Q
ln a
t) = i∈NΠ P − vii,j ≥ t = e−cΠ t . Therefore, the sum
Pk
v Π ) follows the gamma distribution Γ(k, cΠ ). Based
j=1 yj (~
on the above observation, Lemiesz’s algorithm estimates
the weighted cardinality cΠ as Pk k−1
. The proposed
vΠ )
j=1 yj (~
sketch is mergeable, which facilitates efficiently estimating
the weighted cardinality of a sequence represented as a

joint of different sequences Π1 , . . . , Πd . Specifically, given the
sketches of all Π1 , . . . , Πd , the sketch of the joint sequence Π
is computed as:

yj (~v Π ) = min yj (~v Πl ).
l=1,...,d

Therefore, we only need to compute and gather the sketches
of all sequences Π1 , . . . , Πd together, which significantly
reduces the memory usage and communication cost.
To compute the Gumbel-Max sketches of a large collection of vectors (e.g., bag-of-words representations of documents), the straightforward method first instantiates variables ai,1 , . . . , ai,k from UNI(0, 1) for each index i = 1, . . . , n.
Then, for each non-negative vector ~v , it enumerates each i ∈
ln a
ln a
N~v+ and computes − vii,1 , . . . , − vii,k . The above method
requires memory space O(nk) to store all [ai,j ]1≤i≤n,1≤j≤k ,
and time complexity O(kn~+
v ) to obtain the Gumbel-Max
+
sketch of each vector ~v , where n~+
v = |N~
v | is the cardinality
+
of set N~v . We note that k is usually set to be hundreds or
even thousands [6], [7], [16]. Therefore, the straightforward
method costs a huge amount of memory space and time
when the vector of interest has a large dimension, e.g.,
n = 109 . To reduce the memory cost, one can easily use
hash techniques or random number generators with specific
seeds (e.g., consistent random number generation methods
in [17], [18], [19]) to generate each of ai,1 , . . . , ai,k on the
fly, which does not require to calculate and store variables
[ai,j ]1≤i≤n,1≤j≤k in memory.
To address the computational challenge, in this paper, we
propose a novel method FastGM to fast compute a GumbelMax sketch, which reduces the time complexity of computing
+
the sketch from O(kn~+
v ) to O(k ln k + n~
v ). From the example
in Fig. 1, we find two interesting observations in computing
the Gumbel-Max sketch in a straightforward way: 1) The k
ln a
ln a
variables − vii,1 , . . . , − vii,k of a relatively larger element
vi in the ~v are more likely to be the Gumbel-Max variables,
such as the v1 = 0.3 and v5 = 0.2; 2) Each Gumbel-Max variable occurs as one of an element vi ’s top minimal variables,
e.g., all three Gumbel-Max variables (red ones in the first
row) appeared in v1 ’s Top-4 minimal variables. Therefore,
we prioritize the generation order of all kn variables and
reduce the number of generated variables for computing
the Gumbel-Max sketch. The basic idea behind our FastGM
can be summarized as follows. For each element vi > 0
ln a
ln a
in ~v , we generate k random variables − vii,1 , . . . , − vii,k
in ascending order. As shown in Fig. 2, we can generate
a sequence of k tuples (ti,1 , πi,1 ) , . . . , (ti,k , πi,k ), where
ln ai,π

ti,j = − vi i,j , ti,1 < · · · < ti,k and πi,j = ij , (i1 , . . . , ik )
is a random permutation of integers 1, . . . , k . When we are
able to compute the Gumbel-Max sketch of ~v by obtaining
the k random variables in ascending order, it is easy to find
once the current obtained ti,j in tuple (ti,j , πi,j ) is larger
than all elements in the ~
y (~v ), there is no need to obtain the
following tuples (ti,j+1 , πi,j+1 ) , · · · , (ti,k , πi,k ) because they
have no chance to change the Gumbel-Max sketch of ~v . Based
on this property, we model the procedure of computing the
Gumbel-Max sketch as a queuing model with k -servers and
n-queues of different arrival rates. Specifically, each queue
has k customers and each customer randomly selects a server.
A server j = 1, . . . , k just serves the first arrived customer
and ignores the other arrived customers. In addition, a server

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

⃗v

···

j =1
i=1 7.33

3

j =8

j=1 7.33

0.51 5.48 0.17 0.37 6.46 7.86 2.16
0.3
8.98
31.38
2.82 6.98 5.40 5.01 11.30 11.23
0.1
2.46
7.37
21.21
5.62 10.80 27.37 5.24 3.68
0.05
12.02
0.42
17.42
5.96 3.13 22.53 29.04 4.21
0.05 · · ·
4.98
6.31
9.30
0.92 16.78 4.56 1.50 2.93
0.2
22.41
28.74
2.85
26.50
1.74 12.86 5.24 19.08
0.07
8.19
3.78
4.70
7.82
8.97 9.51 1.93 3.85
0.1
1.49
3.16
10.10
16.04
98.46
16.49 6.29 48.46
0.03 i=8
the matrix

h

ln a
− v i,j
i

i

0.42 2.82 0.17 0.37 4.56 1.50 2.16

Fig. 1: An example of computing the Gumbel-Max sketch
~y (~v ) and ~s(~v ) of length k = 8 for a vector ~v , of which
elements yj (~v ) and sj (~v ) respectively record the smallest
element value (i.e., the red one) and its index in the j -th
column of the matrix.

j only records the arrival time and the queue number (i.e.,
from which queue the customer comes) of its first arrived
customer as yj (~v ) and sj (~v ), j = 1, . . . , k respectively, which
have the same probability distributions as the variables yj (~v )
and sj (~v ) defined in Eq. (2) and Eq. (1). When each of the
servers has processed its first arrived customer, we close all
queues and obtain the Gumbel-Max sketch yj (~v ) and sj (~v )
of ~v . Based on the above model, we propose FastGM to fast
compute the Gumbel-Max sketch. We summarize our main
contributions as:
• We introduce a simple queuing model to interpret the

procedure of computing the Gumbel-Max sketch of vector
~v . Using this stochastic process model, we propose a novel
algorithm, called FastGM, to reduce the time complexity
of computing the Gumbel-Max sketch (s1 (~v ), . . . , sk (~v ))
+
and (y1 (~v ), . . . , yk (~v )) from O(n~+
v k) to O(k ln k + n~
v ),
which is achieved by avoiding calculating all k variables
ln a
ln a
− vii,1 , . . . , − vii,k for each i ∈ N~v+ .
• We conduct experiments on a variety of real-world datasets
for applications including probability Jaccard similarity
estimation and weighted cardinality estimation. The experimental results demonstrate that our method FastGM
is orders of magnitude faster than the state-of-the-art
methods without incurring any additional cost.
The rest of this paper is organized as follows. Section 2
and Section 3 present our method FastGM and its extension
Stream-FastGM for non-streaming and streaming settings
respectively. The performance evaluation and testing results
are presented in Section 4. Section 5 summarizes related
work. Concluding remarks then follow.

2

j=3 5.48
j=4 0.17
j=5 0.37

O UR M ETHOD FAST GM

In this section, we first introduce the basic idea behind
our method FastGM through a simple example. Then, we
elaborate on FastGM in detail and discuss its space and time
complexities.

j=6

6.46

j=7

7.86

j=8

2.16

the 1-st row

of matrix in Fig. 1

1≤i≤8,1≤j≤8

⃗s(⃗v ) 0.800 0.400 0.200 0.100 0.100 0.500 0.500 0.100
⃗
y (⃗v ) 1.49

j=2 0.51

0 → t0
0.17→ t1
0.37→ t2
0.51→ t3
2.16→ t4
5.48→ t5
6.46→ t6
7.33→ t7
7.86→ t8

∁1 π ←4
1

∁2 π2←5
∁3 π3←2
∁4 π4←8
∁5 π5←3
∁6 π6←6
∁7 π7←1
∁8 π8←7

Q1

Fig. 2: An example of building a queue Q1 from 8 random
variables in the 1-st row of matrix in Fig. 1. We use { to
represent a customer in the queue.

2.1

Basic Idea

In Fig. 1, we provide an example of generating a Gumbel-Max
sketch of a vector ~v = (0.3, 0.1, 0.05, 0.05, 0.2, 0.07, 0.1, 0.03)
to illustrate our basic idea, where we have n = 8 and
k = 8. Note that we aim to fast compute each yj (~v )
ln a
and sj (~v ), where yj (~v ) = min1≤i≤8 − vii,j and sj (~v ) =
ln a

argmin1≤i≤8
− ivii,j , 1 ≤ j ≤ 8, i.e., in each column j of
h

ln ai,j
yj (~v ) records the minimum
vi
1≤i≤8,1≤j≤8
element and sj (~vh) records
the
index of this element. We
i
ln ai,j
generate matrix − vi
based on the tradi1≤i≤8,1≤j≤8

matrix −

tional Gumbel-Max Trick and mark the minimum element
(i.e., the red one indicating the Gumbel-Max variable) in
each column j . We find that Gumbel-Max variables tend to
equal index i with large weight vi . For example, among
the values of all Gumbel-Max variables s1 (~v ), . . . , s8 (~v ),
index 1 with v1 = 0.3 appears 3 times, while index 3
with v3 = 0.05 never occurs. Based on the above observations, we prioritize the generation order of the 64 variables
ln a
− vii,j , 1 ≤ i ≤ 8, 1 ≤ j ≤ 8 according to their values. We
first respectively select Ri smallest variables from each row i
to compute the Gumbel-Max sketch, where Ri is proportional
to the weight vi . The total number R = R1 + . . . + Rn of
variables from all rows is computed as R = k ln k . This is the
expected number of generated variables before each column
j has at least one variable. To some extent, it is similar to the
Coupon collector’s problem [20]. Specifically, in the example of
Fig. 1, we have R = 17 = d8×ln 8e, and each Ri is computed
as Ri = dRvi∗ e, where ~v ∗ = (v1∗ , . . . , v8∗ ) is the normalized
vector of ~v . We have R1 = 6, R2 = 2, R3 = 1, R4 = 1,
R5 = 4, R6 = 2, R7 = 2, and R8 = 1. Meanwhile, we find
that each Gumbel-Max variable occurs as one of a row i’s
Top-Ri minimal elements. For example, the two GumbelMax variables occurring in the 5-th row are all among the
Top-R5 (i.e., Top-4) minimal elements. Moreover, we easily
ln a
ln a
observe that k random variables − vii,1 , . . . , − vii,k in each
row indeed are k independent random variables follow
the exponential distribution EXP(vi ). Therefore, we can
generate these k variables in ascending order by exploiting
the distribution of the order statistics of exponential random
variables. Based on the above insights, we derive our method
FastGM. As the example in Fig. 2, for each row, we construct
such a queue Qi with arrival rate vi for the k variables drawn

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

from the distribution EXP(vi ) according to their values. Then,
we first compute the variables that are in the front of the
queues or in the queues with large arrival rates vi , because
they are smaller ones among all kn variables and are more
likely to become the Gumbel-Max variables. Moreover, we
early stop a queue when its remaining variables have no
chance to be the Gumbel-Max variables. Also take Fig. 1 as
an example. Compared with the straightforward method
computing all nk = 64 random variables, we compute
s1 (~v ), . . . , sk (~v ) by only obtaining Top-Ri minimal elements
of each row i, which
P8 significantly reduces the computation
cost to around i=1 Ri = 19. In summary, our method
FastGM efficiently computes the Gumbel-Max sketch ~
y (~v )
and ~s(~v ) of vector ~v through managing the number and
ln a
order of variables − vii,j belonging to different elements vi .
Specifically, we aim to fast search and compute those variables
that have a high probability to become the elements of the
Gumbel-Max sketch, and fast prune variables have no chance
to be an element in the Gumbel-Max sketch. In the following,
when no confusion arises, we simply write sj (~v ), yj (~v ) and
+
n~+
v as sj , yj and n respectively.

(i.e., records this customer’s arrival time and the index of
queue i, same as Eq. (2) and Eq. (1)). Then, we naturally have
the following two fundamental questions for the design of
FastGM:
Question 1. How to fast search customers with the smallest
arrival time to become candidates for the servers from these
N~v+ queues?
Question 2. How to early stop a queue Qi , i ∈ N~v+ ?
We first discuss Question 1. We note that customers
of different queues Qi arrive at different rates vi . Recall
the example in Fig. 1, the basic idea behind the following
technique is that queue Qi with a high rate vi is more likely
to produce customers with the smallest arrival time (i.e.
Gumbel-Max variables). Especially, when z customers have
arrived, let ti,z denote the arrival time of the z -th customer in
queue Qi . We find that ti,z can be represented as the sum of
z identically distributed exponential random variables with
mean kv1 i (another perspective can be found in paper [1]).
Therefore, the expectation and variance of variable ti,z are
computed as

2.2

We easily find that E(ti,z ) is l times smaller than E(tj,z ) when
vi is l times larger than vj .
To obtain the first R customers of the joint of all queues
Qi , i ∈ N~v+ , we let each queue Qi release Ri = dRvi∗ e
customers,P
where ~v ∗ is the normalized vector of ~v . Then, we
n
have R ≈ i=1 Ri . For all i ∈ N~v+ , their ti,Ri approximately
have the same expectation.

Fast Gumbel-Max Sketch Generation

Our FastGM first constructs a queue Qi for variables in
each row as shown in Fig. 2. Based on this, we propose two
modules: FastSearch for efficiently searching small variables
in each queue, and FastPrune for pruning overlarge queues
that cannot contribute to the Gumbel-Max sketch. Before
introducing our FastGM in detail, we first illustrate how to
build a queue Qi and model the procedure of computing the
Gumbel-Max sketch from another perspective via a Queuing
Model with k -servers and n-queues.
Queuing Model with k -servers and n-queues. In Fig. 2,
we show how to construct a queue Qi where k random
ln a
ln a
variables − vii,1 , · · · , − vii,k of vi are sorted in ascending
order. For simplicity, we define a variable bi,j as:

bi,j =

− ln ai,j
,
vi

i = 1, · · · , n j = 1, · · · , k.

(3)

We easily observe that bi,1 , . . . , bi,k are equivalent to k independent random variables generated according to the exponential distribution EXP(vi ). Let bi,(1) < bi,(2) < . . . < bi,(k)
be the order statistics corresponding to variables bi,1 , . . . , bi,k .
We construct each queue Qi with k customers {i,j whose
arrival time ti,j , bi,(j) , j = 1, . . . , k and each customer randomly selects a server
 j . Specifically,we generate a sequence
of k tuples bi,(1) , i1 , . . . , bi,(k) , ik , where (i1 , . . . , ik ) is a
random permutation of integers 1, . . . , k and denotes the
server sequence randomly selected by customers in a queue
Qi . It is easy to observe that values (resp. positions) of
element vi ’s k variables are the customers’ arrival time
(resp. selected servers) in the queue Qi . Accordingly, we
assign arrival time ti,j , bi,(j) and selected server
πi,j , ij

for each customer, i.e., (ti,j , πi,j ) , bi,(j) , ij , j = 1, . . . , k .
Note that, k variables bi,1 , . . . , bi,k follow EXP(vi ) with a rate
parameter vi . Therefore, customers in queue Qi also arrive
at this rate vi . As shown in Fig. 3, based on the built queues,
the procedure of computing the Gumbel-Max sketch can
be modeled as a Queuing Model with k -servers and n-queues,
where each server only serves the first arrived customer

E(ti,z ) =

z
,
kvi

E(ti,Ri | R) ≈

Var(ti,z ) =

k

R
Pn

j=1 vj

,

z
.
k 2 vi2

i ∈ N~v+ .

(4)

(5)

Therefore, the R customers with the smallest arrival time are
expected to be released.
Next, we discuss Question 2, which is inspired by the
generation of ascending-order random variables. For an
element with index j in the Gumbel-Max Sketch, we use
two registers yj and sj to keep track of information on
the customer with the smallest arrival time among all the
released customers selected by server j , where yj records
the customer’s arrival time and sj records the index of the
queue this customer comes from, i.e., Qsj . When all servers
1, . . . , k have been selected by at least one customer, we let
y ∗ keep track of the maximum value of y1 , . . . , yk , i.e.,

y ∗ = max yj .
j=1,...,k

Then, we can stop queues Qi when a customer coming from
Qi has an arrival time larger than y ∗ because the arrival time
of the subsequent customers from Qi is also larger than y ∗ ,
which will not change any y1 , . . . , yk and s1 , . . . , sk .
Based on the above two discussions, we develop our
method FastGM to fast generate a k -length Gumbel-Max
sketch with ~s~v = (s1 , . . . , sk ) and ~
y~v = (y1 , . . . , yk ) of any
non-negative vector ~v . As shown in Fig. 3, FastGM consists
of two modules: FastSearch and FastPrune. FastSearch is designed to quickly search customers with the smallest arrival
time coming from all queues Q1 , . . . , Qn and check whether
all servers 1, . . . , k have received at least one appointment
from customers (i.e., selected by at least one customer). When

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

vi∗ = Pnvi vi
i=1

server1

server2

server3

server4

server5

y1 s 1

y2 s 2

y3 s 3

y3 s 3

y3 s3

⌈Rv4∗ ⌉=1

⌈Rv5∗ ⌉=2

⌈Rv6∗ ⌉=1

∁1

∁1
∁2

∁1

∁3
∁4
∁5

∁2
∁3
∁4
∁5

⌈Rv1∗ ⌉=2

⌈Rv2∗ ⌉=1

∁1
∁2

∁1

∁3
∁4
∁5

∁2
∁3
∁4
∁5

y ∗ = max yj
j=1,...,k

5

Q1

Q2

⌈Rv3∗ ⌉=3

∁1
∁2
∁3
∁4
∁5

Q3

∁2
∁3
∁4
∁5
Q4

Q5

FastSearch Module
FastPrune Module

Q6

Fig. 3: Illustration of our FastGM. The FastSearch module first selects a number of customers { from each queue as candidates
(i.e., the blue ones) and uses their arrival time to compute the y ∗ = maxj=1,...,k yj . Then, the FastPrune module closes a
queue according to the arrival time of the next customer (i.e., the red one) in each queue compared with the value of y ∗ , if y ∗
is smaller, we close the queue for the rest of customers (i.e., the gray ones), which have no chance to be the first customers
for any servers.
no servers are unreserved, we start the FastPrune module to to iteratively produce a random permutation i1 , . . . , ik for
close each queue Qi , i ∈ N~v+ . We perform the procedure of integers 1, . . . , k . For an array (πi,1 , . . . , πi,k ) with elements
FastPrune because following customers coming from Qi may (1, . . . , k), in each step z , 1 ≤ z ≤ k , this method randomly
also have an arrival time smaller than y ∗ and the customers selects a number iz from z, z + 1, . . . , k and swaps the two
may become the first arrived customers for some servers j elements in the array with indices z and iz . To build the
and change the values of yj and sj after the procedure of queue Qi , we assign bi,(z) and the element with index z
FastSearch. Before we introduce these two modules in detail, in the array to the arrival time and selected server of z -th
we first elaborate on the method of generating exponential customer, respectively, i.e.,
random variables in ascending order, which is a building
ti,z ← bi,(z) ,
iz ← πi,z .
block for both modules.
• Generating Ascending Exponential Random Variables: We easily find that k variables bi,(1) , · · · , bi,(k) shuffled by the
Next we detail how to sequentially generate k random random permutation i1 , . . . , ik have the same distribution as
variables bi,1 , . . . , bi,k in ascending order for each positive the variables bi,1 , · · · , bi,k generated in a direct manner.
element vi of vector ~v (Lines 9-14 and Lines 24-29 in Algo- • FastSearch Module: This module fast searches customers
rithm 1). As we mentioned, these bi,1 , . . . , bi,k are random with the smallest arrival time, and consists of the following
variables according to the exponential distribution EXP(vi ), steps:
i.e.,
Step 1: Iterate on each i ∈ N~v+ and repeat to generate
bi,j ∼ EXP(vi ), j = 1, . . . , k.
(6)
dRvi∗ e exponential variables (i.e., the arrival time
of customers) in ascending order (Lines 9-14 in
Let bi,(1) < bi,(2) < . . . < bi,(k) be the order statistics
Algorithm
1). Meanwhile, each server j uses registers
corresponding to variables bi,1 , . . . , bi,k . Alfré Rényi [21]
y
and
s
to keep track of information of the first
j
j
observes that each bi,(z) , z = 1, . . . , k satisfies
arrived customer, where yj records the customer’s
!
z
1 X − ln ui,n
arrival time and sj records the index of the queue
bi,(z) ∼
,
(7)
where the customer comes from (Lines 15-18 in
vi n=1 k − n + 1
Algorithm 1);
where all variables ui,1 , . . . , ui,z ∼ UNI(0, 1) are indepen- Step 2: If there remain any unreserved servers, we increase
dent random variables. Note that − ln ui,n is an EXP(1)
R by ∆ and then repeat Step 1. Otherwise, we stop
distributed random variable. Therefore, one easily obtains
the FastSearch procedure.
the following equation:
For
simplicity,
we set the parameter ∆ = k . In our experi

1
− ln ui,z
ments, we find that the value of ∆ has a small effect on the
bi,(z) − bi,(z−1) ∼
, 2 ≤ z ≤ k. (8)
performance of FastGM.
vi k − z + 1
Based on the above observation, we generate the order • FastPrune Module: When all servers 1, . . . , k have been
statistics bi,(1) , · · · , bi,(k) for each element vi of vector ~v in selected by at least one customer among all the released
customers. We start the FastPrune module, which mainly
an iterative way as:
consists of the following two steps:


1
− ln ui,z
Step 1. Compute y ∗ = maxj=1,...,k yj .
bi,(z) ← bi,(z−1) +
, 1 ≤ z ≤ k,
vi k − z + 1
Step 2. For each Qi , i ∈ N~v+ , we repeat to compute the next
where bi,(0) = 0. In addition, we use the Fisher-Yates
customer’s arrival time (Lines 24-29 in Algorithm 1).
shuffle [22] (Lines 11-12 and lines 26-27 in Algorithm 1)
Once a customer’s arrival time is larger than y ∗ , we

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

stop releasing customers from queue Qi (Lines 3032 in Algorithm 1). As we mentioned, variables yj
and sj keep track of information of the first arrived
customer. Therefore, y1 , . . . , yk and s1 , . . . , sk may
also be updated by receiving new appointments from
newly released customers with arrival times smaller
than y ∗ at this step (Lines 33-36 in Algorithm 1).
Therefore, y ∗ may also decrease with the number of
released customers, which accelerates the termination
of all queues Qi , i ∈ N~v+ .
2.3

Mergeability

For some applications, the dataset of interest Π is distributed
over multiple sites. Suppose that there are r sites, each site
i = 1, . . . , r holds a sub-dataset Πi . Each site i can compute
the Gumbel-Max sketch (~s(i) , ~
y (i) ) of its set N (i) , which is
the set of objects appearing in Πi . Here set N (i) can be easily
represented as a weighted vector following the weighted
cardinality estimation discussed in Section 1 and its GumbelMax sketch (~s(i) , ~
y (i) ) can be computed based on our
method FastGM. A central site can collect all sites’ sketches
(~s(1) , ~y (1) ), . . . , (~s(r) , ~y (r) ) and then use them to compute the
Gumbel-Max sketch (~s∪ , ~
y ∪ ) of the union set N (1) ∪· · ·∪N (r) .
For the sketch of union set, each element yi∪ , j = 1, . . . , k of
(i)
(i)
~y ∪ is computed as yj∪ = mini=1,...,r yj , where yj is the j th element of vector ~
y (i) . Each element s∪
s∪ is computed
i of ~
(i∗ )
(i)
(i∗ )
∪
∗
as sj = sj , where i = argmini=1,...,r yj and sj is the
∗
j -th element of vector ~s(i ) . At last, the weighted cardinality
of dataset Π can be estimated from the above Gumbel-Max
sketch (~s, ~
y ).

6

Algorithm 1: Pseudo code of our FastGM.
Input : ~v = (v1 , . . . , vn )
Output : ~s = (s1 , . . . , sk ), ~
y = (y1 , . . . , yk )

R ← 0; k ∗ ← k ; (y1 , . . . , yk ) ← (−1, . . . , −1);
+
2 foreach i ∈ N~
v do
3
(bi , zi ) ← (0, 0); (πi,1 , . . . , πi,k ) ← (1, . . . , k);

1

/* The following part is FastSearch

while k ∗ 6= 0 do
5
R ← R + ∆;
6
foreach i ∈ N~v+ do
7
Ri ← dRvi∗ e;
8
while zi < Ri do
9
zi ← zi + 1;

*/

4

10
11

12

/* Variable u ∼ UNI(0, 1).

*/

u ← RandUNI
 (0, 1, seed
 ← i||zi );
1
ln u
bi ← bi − vi k−zi +1 ;

/* RandInt(zi , k) returns a number from
{zi , zi + 1, . . . , k} at random.
*/

j ← RandInt(zi , k);

/* Swap(πi,zi , πi,j ) exchanges the values
of two variables πi,zi and πi,j .
*/
13
14
15
16
17
18

Swap(πi,zi , πi,j );
c ← πi,zi ;
if yc < 0 then
(yc , sc ) ← (bi , i); k ∗ ← k ∗ − 1;
else if bi < yc then
(yc , sc ) ← (bi , i);

/* The following part is FastPrune

j ∗ ← argmaxj=1,...,k yj ; N ← N~v+ ;
20 while N is not empty do
21
R ← R + ∆;
22
foreach i ∈ N do
23
while zi < Ri do
24
zi ← zi + 1;
25
u ← RandUNI
 (0, 1, seed
 ← i||zi );
1
ln u
26
bi ← bi − vi k−zi +1 ;
27
j ← RandInt(zi , k);
28
Swap(πi,zi , πi,j );
29
c ← πi,zi ;
30
if bi > yj ∗ then
31
N ← N \ {i};
32
break;

*/

19

2.4

Error Analysis

As aforementioned, the parts ~s(~v ) and ~
y (~v ) of Gumbel-Max
sketches produced by FastGM are equivalent to the sketches
proposed in [6] and [8], respectively. Therefore, we have the
following error analysis results.
Theorem 1. [6] When using the part ~s(~v ) of Gumbel-Max sketch
to estimate the probability Jaccard similarity JP (~
u, ~v ) between ~u
and ~v , the expectation and variance of estimation JˆP (~
u, ~v ) are


E JˆP (~
u, ~v ) = JP (~u, ~v ),


1
Var JˆP (~
u, ~v ) = JP (~u, ~v ) (1 − JP (~u, ~v )) .
k
Theorem 2. [8] When using the part ~
y (~v ) of Gumbel-Max sketch
to estimate the weighted cardinality cΠ of a sequence Π, the
expectation and variance of estimation ĉΠ are
E (ĉΠ ) = cΠ ,

33
34
35
36

if bi < yc then
(yc , sc ) ← (bi , i);
if c == j ∗ then
j ∗ ← argmaxj=1,...,k yj ;

Var (ĉΠ /cΠ ) = 2/k + O(1/k 2 ) ≈ 2/k.
2.5

Space and Time Complexities

Space Complexity. For a non-negative vector ~v with n~+
v
positive elements, our method FastGM requires k log k bits
to store the (πi,1 , . . . , πi,k ) of each i ∈ N~v+ , and in summary,
n~+
v k log k bits are desired. In addition, 64k bits are desired
for storing y1 , . . . , yk (we use 64-bit floating-point registers
to record y1 , . . . , yk ), and k log n bits are required for storing

s1 , . . . , sk , where n is the size of the vector. However, the
additional memory is released immediately after computing
the sketch and is far smaller than the memory for storing
the generated sketches of massive vectors (e.g. documents).
Therefore, FastGM requires n~+
v k log k + 64k + k log n bits
when generating a k -length Gumbel-Max sketch ~s(~v ) =

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

(s1 , . . . , sk ) and ~y (~v ) = (y1 , . . . , yk ) of ~v .
Time Complexity. We easily find that a non-negative
vector and its normalized vector have the same GumbelMax sketch. For simplicity, therefore we analyze the time
complexity of our method only for normalized vectors. Let
~v ∗ = (v1∗ , . . . , vn∗ ) be a normalized and non-negative vector.
Define a variable ỹ ∗ as:
ỹ ∗ = max ỹj ,
j=1,...,k

ln a
where ỹj = mini∈N +∗ − v∗i,j , j = 1, . . . , k. At the end of
i
~
v

our FastPrune procedure, we easily find that each register
yj used in the procedure equals ỹj and register y ∗ equals
ln a
ỹ ∗ . Because − v∗i,j ∼ EXP(vi∗ ), we easily find that each
i
P
yj follows the exponential distribution EXP( ni=1 vi∗ ), i.e.
EXP(1). From [23], we have
E(ỹ ∗ ) =

k
X
1
m=1

Var(ỹ ∗ ) =

m

≤ ln k + γ,

∞
k
X
X
1
π2
1
<
=
,
m2
m2
6
m=1
m=1

where γ = 1. From Chebyshev’s inequality, we have


q
1
P |ỹ ∗ − E(ỹ ∗ )| ≥ α Var(ỹ ∗ ) ≤ 2 .
α
p
Therefore, ỹ ∗ ≤ E(ỹ ∗ ) + α Var(ỹ ∗ ) happens with a high
probability when α is large. In other words, theprandom
variable ỹ ∗ can be upper bounded by E(ỹ ∗ ) + α Var(ỹ ∗ )
with a high probability. Next, we derive the expectation of
ti,R after the first R customers have been released. For each
queue Qi , i ∈ N~v+ , from Eqs. (4) and (5), we find that the last
customer among these first R customers has a timestamp
ti,Ri with the
≈ R
k . When R =
p expectation E(ti,Ri | R)
απ
∗
∗
√
k(E(ỹ ) + α Var(ỹ )) < k(ln k + γ + 6 ), the probability of
E(ti,Ri ) > ỹ ∗ is almost 1 for large α, e.g., α > 10. Therefore,
we find that after the first O(k ln k) customers, each queue
Qi is expected to be early terminated and so we are likely to
acquire all the Gumbel-Max variables. We also note that each
positive element has to be enumerated once in the FastPrune
model. Therefore, the total time complexity of our method
FastGM is O(k ln k + n~+
v ).

3

O UR M ETHOD S TREAM -FAST GM

We extend our method FastGM to handle data streams.
Given a stream Π represented as a sequence of elements
i ∈ {1, . . . , n}. An element i may occur multiple times in Π
and it has a fixed weight vi . Our method Stream-FastGM is
a fast one-pass algorithm for computing the Gumbel-Max
sketch of Π, which reads and processes each element arriving
at the stream exactly once.
The pseudo-code of Stream-FastGM is shown in Algorithm 2. Similar to FastGM, for each server j = 1, . . . , k ,
we use two registers yj and sj to record its first customer’s arrival time and queue number. In addition, we use
y ∗ = maxj=1,...,k yj to record the maximum of all y1 , . . . , yk .
As we mentioned, the FastPrune procedure can be used
only after each of the servers has been selected by at least
one customer. We use a flag F lagF astP rune to indicate

7

Algorithm 2: Pseudo code of our Stream-FastGM.
Input : data stream Π
Output : ~s = (s1 , . . . , sk ), ~
y = (y1 , . . . , yk )

k ∗ ← k ; j ∗ ← 1; (y1 , . . . , yk ) ← (−1, . . . , −1);
2 foreach element i in stream Π do
3
b ← 0; (π1 , . . . , πk ) ← (1, . . . , k);
4
for l = 1, . . . , k do
5
u ← RandUNI(0, 1, seed ← i||l);
/* vi is the
*/
 weight
 of element i.
ln u
6
b ← b − v1i k−l+1
;
7
j ← RandInt(l, k);
8
Swap(πl , πj );
9
c ← πl ;
10
if FlagFastPrune==False then
11
if yc < 0 then
12
(yc , sc ) ← (b, i);
13
k ∗ ← k ∗ − 1;
14
if k == 0 then
15
F lagF astP rune ← T rue;
16
j ∗ ← argmaxj=1,...,k yj ;
1

17
18
19
20
21
22
23
24
25

else if b < yc then
(yc , sc ) ← (b, i);
if FlagFastPrune==True then
if b > yj ∗ then
break;
if b < yc then
(yc , sc ) ← (b, i);
if c == j ∗ then
j ∗ ← argmaxj=1,...,k yj ;

whether the FastPrune procedure can be used. For each
element i arriving at stream Π, we repeat to generate random
exponential variables in ascending order. When the flag
F lagF astP rune is true and the generated variable has a
value larger than y ∗ , we stop processing the current element.

4

E VALUATION

We evaluate our method FastGM with the state-of-theart on two tasks: (Task 1) probability Jaccard similarity
estimation and (Task 2) weighted cardinality estimation.
All algorithms run on a computer with a Quad-Core Intel(R)
Xeon(R) CPU E3-1226 v3 CPU 3.30GHz processor. To demonstrate the reproducibility of the experimental results, we
make our source code publicly available1 .
4.1

Datasets

For the task of probability Jaccard similarity estimation, we
verify the efficiency of our FastGM in generating GumbelMax sketch with different lengths k ∈ {26 , 27 , . . . , 212 } for
vectors of length in the range n ∈ {102 , 103 , 104 }. We generate the weights of synthetic vectors according to the uniform
1. https://github.com/YuanmingZhang05/FastGM

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

TABLE 1: Statistics of used real-world datasets.
Dataset
Real-sim [24]
Rcv1 [25]
News20 [26]
Libimseti [27]
Wiki10 [28]
MovieLens [29]

#Vectors
72,309
20,242
19,996
220,970
14,146
69,878

#Features
20,958
47,236
1,355,191
220,970
104,374
80,555

distribution UNI(0, 1) and the exponential distribution with
rate 1 EXP(1). In addition, we also run experiments on six
real-world datasets: Real-sim [24], Rcv1 [25], News20 [26],
Libimseti [27], Wiki10 [28], and MovieLens [29]. In detail,
Real-sim [24], Rcv1 [25], and News20 [26] are datasets of
web documents from different sources where each vector
represents a document and each entry in the vector refers
to the TF-IDF score of a specific word for the document.
Libimseti [27] is a dataset of ratings between users on the
Czech dating site, where each vector refers to a user and each
entry records the user’s rating to another one. Wiki10 [28] is
a dataset of tagged Wikipedia articles, where each vector and
element represent an article and a tag, respectively. Moreover,
the weight of an element indicates how relevant the tag is
for the article. MovieLens [29] is a dataset of movie ratings,
where each vector is a user and each entry in the vector is
that user’s rating for a specific movie. The statistics of all the
above datasets are summarized in Table 1.
As for the task of weighted cardinality estimation, we follow the experimental settings in [8]. We conduct experiments
on both synthetic datasets and a simulated scenario obtained
from real-world problems. In the later Section 4.5, we detail
them. In addition, we also design a data steaming setting
to demonstrate the mergeability of our Gumbel-Max sketch
and the performance of our Stream-FastGM. Specifically, we
generate a set of elements arriving in a streaming fashion.
4.2

Baseline

To demonstrate the improvement of our FastGM over the
conference version of FastGM (in short, FastGM-c), we also
apply FastGM-c as a baseline in efficiency experiments.
For task 1, probability Jaccard similarity estimation, we
compare our method FastGM with P -MinHash [6]. To
highlight the efficiency of FastGM, we further compare
FastGM with the state-of-the-art weighted Jaccard similarity
estimation method, BagMinHash [30], which is used for
estimating weighted Jaccard similarity JW . The weighted
Jaccard similarity JW that BagMinHash aims to estimate
is an alternative similarity metric to the probability Jaccard
similarity JP we focused on in this paper. Experiments and
theoretical analysis in [6] have shown that weighted Jaccard
similarity JW and probability Jaccard similarity JP usually
have similar performance on many applications such as fast
searching similar set. Notice that BagMinHash estimates a
different similarity metric and thus we only show its results
on efficiency. For task 2, weighted cardinality estimation,
we compare our method with Lemiesz’s sketch [8].
4.3

Metric

For both tasks of probability Jaccard similarity estimation
and weighted cardinality estimation, we use the running

8

time and root mean square error (RMSE) to measure our
method’s efficiency and effectiveness, respectively. In detail,
we measure the RMSEs of probability Jaccard similarity
estimation Jˆ and weighted cardinality estimation ĉ with
respect to their true values J and c as:
q
q
ˆ = E((Jˆ − J)2 ), RMSE(ĉ) = E((ĉ − c)2 ).
RMSE(J)
All experimental results are empirically computed from 1,000
independent runs by default.
4.4

Probability Jaccard Similarity Estimation

We conduct experiments on both synthetic and real-world
datasets for the task of probability Jaccard similarity estimation. Specially, we first use synthetic weighted vectors
to evaluate the performance of FastGM for vectors with
different dimensions. Then, we show results on 6 real-world
datasets.
Results on synthetic vectors. We first conduct experiments on weighted vectors with uniform-distribution
weights. Without loss of generality, we let n~+
v = n for
each vector, i.e., all elements of each vector are positive.
As shown in Fig. 4 (a), (b) and (c), when n = 103 , FastGM
is 13 and 22 times faster than BagMinHash and P -MinHash
respectively. As n increases to 104 , the improvement becomes
8 and 125 times respectively. Especially, the sketching time
of our method is around 0.02 seconds when n = 104 and
k = 212 , while BagMinHash and P -MinHash take over 0.15
and 2.5 seconds for sketching respectively. In Fig. 4 (d),
(e), and (f), we show the running time of all competitors for
different n. Our method FastGM is 13 to 100 times faster than
P -MinHash for different n. Compared with BagMinHash,
FastGM is about 60 times faster when n = 1, 000, and is
comparable as n increases to 100, 000. It indicates that our
method FastGM significantly outperforms BagMinHash for
vectors having less than 100, 000 positive elements, which
are prevalent in real-world datasets. As shown in Fig. 4,
our FastGM is consistently faster than FastGM-c, when
n = 100 and n = 1, 000 FastGM is around 1.2 and 1.5
times faster than FastGM-c, respectively. Results are similar
when the weights of synthetic vectors follow the exponential
distribution EXP(1), thus we omit them here.
Results on real-world datasets. Next, we show results
on the real-world datasets in Table 1. We report the sketching
time of all algorithms in Fig. 5. We see that our method outperforms P -MinHash and BagMinHash on all the datasets.
FastGM is consistently faster than FastGM-c, especially on
datasets Rcv1, Libimesti, and MovieLens, FastGM is 4 times
faster than FastGM-c on average. On sparse datasets such
as Real-sim, Rcv1, Wiki10, and MovieLens, FastGM is about
8 and 12 times faster than P -MinHash and BagMinHash
respectively. BagMinHash is even slower than P -MinHash
on these datasets. On dataset News20 we note that FastGM
is 26 times faster than P -MinHash.
Fig. 6 shows the estimation error of FastGM and P MinHash on datasets Real-sim and MovieLens. Due to a large
number of vector pairs, we here randomly select 100, 000
pairs of vectors from each dataset and report the average
RMSE. We note that both algorithms give similar accuracy,
which is coincident with our analysis. We omit similar results
on other datasets.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

10−3

10−3

10−4
26

10−4

27

28

29 210 211 212
k

10

−5

26

(a) n = 102

10

BagMinHash
FastGM

28

29 210 211 212
k

(c) n = 10
Time (second)

100

P -MinHash
FastGM-c

103

10

104

105

1

P -MinHash
FastGM-c

P -MinHash
FastGM-c

−1

10−3

105

10−4 2
10

n

103

101
10

0

10−1 6
2

(e) k = 2

104

105

28

100
27

103
101
100

27

28

10

4

P -MinHash
FastGM-c

BagMinHash
FastGM

103
102
101
100 6
2

104

27

28

29 210 211 212
k

29 210 211 212
k

P -MinHash
FastGM-c

BagMinHash
FastGM

103
102
101
100 6
2

(e) Wiki10

27

28

29 210 211 212
k

(f) MovieLens

Fig. 5: (Task 1) Efficiency of FastGM compared with P MinHash, BagMinHash, and the conference version of
FastGM (FastGM-c) for different k on real-world datasets.
1.5

·10−2

RMSE

1
0.5
06
2

3

P -MinHash
FastGM

Weighted Cardinality Estimation

In this task, we compare our FastGM with Lemiesz’s sketch
on both effectiveness and efficiency. The experimental results
show that our FastGM sketch has the same accuracy as
Lemiesz’s sketch and is orders of magnitude faster in producing sketches. In the following, we detail the experiments
on both synthetic datasets and a simulated scenario obtained
from real-world problems in wireless sensor networks.
Results on synthetic datasets. To evaluate the weighted
cardinality estimation accuracy of our method, we generate a
variety of data examples with different cardinalities. We
vary the number of elements in the data examples and
generate the weights of elements according to the uniform
distribution UNI(0, 1) and the normal distribution N (1, 0.1).
We report the RMSEs between the true cardinalities c of data
examples and estimations ĉ from the sketches. As shown
in Fig. 7, our FastGM sketch has the same performance
as Lemiesz’s sketch on each dataset, because the ~
y part of
FastGM and Lemiesz’s sketch have the same results but are
computed in different ways. The efficiency of generating the
two sketches is totally the same as the results reported in
Fig. 4, where Lemiesz’s sketch has the same running time as
P -MinHash. Therefore, in terms of efficiency, our FastGM

29 210 211 212
k

(d) Libimseti

BagMinHash
FastGM

102

10−1 6
2

28

(b) Rcv1

29 210 211 212
k

P -MinHash
FastGM-c

(f) k = 210

Fig. 4: (Task 1) The efficiency of FastGM compared with
P -MinHash, BagMinHash, and the conference version of
FastGM (FastGM-c) on synthetic vectors, where each element
in the vector is randomly selected from UNI(0,1).

4.5

27

n
9

BagMinHash
FastGM

BagMinHash
FastGM

101

10−1 6
2

29 210 211 212
k

P -MinHash
FastGM-c

102

(c) News20

10−2

10−3

28

102

104

BagMinHash
FastGM

100

10

10−2

104

103

(d) k = 28

BagMinHash
FastGM

103

27

n

4

10−1

10−4 2
10

10−4
102

Time (second)

27

100 6
2

103

(a) Real-sim

BagMinHash
FastGM

10−3

10−3

10

10

0

P -MinHash
FastGM-c

10−2

10−2

1

10

1

10−1

−1

10−4 6
2

29 210 211 212
k

101

104

RMSE

10

0

P -MinHash
FastGM-c

28

102

(b) n = 103
Time (second)

Time (second)

101

27

Time (second)

10

−5

BagMinHash
FastGM

Time (second)

10−2

10

P -MinHash
FastGM-c

3

Time (second)

10−2

BagMinHash
FastGM

Time (second)

10−1

P -MinHash
FastGM-c

Time (second)

100

BagMinHash
FastGM

Time (second)

10−1

P -MinHash
FastGM-c

Time (second)

Time (second)

100

9

27

28

29 210 211 212
k

(a) Real-sim

·10−2

P -MinHash
FastGM

2
1
06
2

27

28

29 210 211 212
k

(b) MovieLens

Fig. 6: (Task 1) The accuracy of FastGM compared with
P -MinHash for different k .
sketch outperforms Lemiesz’s sketch by as much as FastGM
outperforms P -MinHash. Hence we omit similar results.
Moreover, in Fig. 8a we show the running time of computing
the sketches by using our Stream-FastGM compared with
Lemiesz’s sketch, and our Stream-FastGM is 23 times faster
than Lemiesz’s sketch on average when n = 1, 000. In Fig. 8b
we report the running time of generating the sketches of
length k = 1024 for data examples with different objects n,
our Stream-FastGM is about 120 times faster than Lemiesz’s
sketch at n = 106 .

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

RMSE

30

80

Lemiesz′s sketch
FastGM

60

20

0
0

200 400 600 800 1,000
n

80

Lemiesz′s sketch
FastGM

200 400 600 800 1,000
n

(b) vi ∼ N (1, 0.1), k = 200
160
120

40

80

20

40
27

28

06
2

29 210 211 212
k

(c) vi ∼ UNI(0, 1), n = 1, 000

27

28

(d) vi ∼ N (1, 0.1), n = 1, 000

10−2

101

Lemiesz′s sketch
Stream-FastGM

100

10−1

10−3
10−4 6
2

102
Time (second)

Time (second)

Lemiesz′s sketch
Stream-FastGM

28

29 210 211 212
k

10−3 3
10

104

sA
3

...

sA
d

sB3

...

sBd

p2
p1

sB2

p1

layer 1

successfully sent to sA
2 in chance p1 , meanwhile a copy of
this traffic packet also has p2 chance to be successfully sent
to node sB2 . Note that p1 does not necessarily equal 1 − p2 .
In the experiment setting, the first node in each sequence
is considered as the source that generates traffic packet i
with size vi in sequence. After each source s1 generates a
sequence Π consisting of n traffic packets, we have a vector
~v Π of length n from this sequence Π. In our experiment,
we follow the setting in [8] and set p1 = 0.9, p2 = 0.1,
d = 30, n = 10, 000 and the sizes of packets vi are generated
according to a Beta distribution with parameters α = β = 5.
Take a node in the second layer sA
2 as an example, traffic
packet sequence received by sA
2 is a mixture of some traffic
packets in sequences ΠsA and ΠsB of both sources sA
1 and
1

10−2

27

p1

29 210 211 212
k

Fig. 7: (Task 2) The weighted cardinality estimation errors
on synthetic datasets, where the lengths k of both Lemiesz’s
and FastGM sketches are the same. For each data example,
we generate ~v Π with n objects of which weights ~vjΠ are
derived according to the uniform distribution UNI(0, 1) and
the normal distribution N (1, 0.1) respectively.
10−1

sB1

sA
2
p2

Fig. 9: An example to show a simulated sensor network
with d layers using the braid chain strategy to transfer traffic
packets, where each node and each edge represent a sensor
in the network and a traffic transfer path, respectively. The
p1 and p2 denote the probability of a successful transmission
between two nodes.

Lemiesz′s sketch
FastGM

RMSE

RMSE

60

p1

p2

20

(a) vi ∼ UNI(0, 1), k = 200

06
2

sA
1
p2

40

10
0
0

Lemiesz′s sketch
FastGM

RMSE

40

10

105
n

106

107

(a) Generating sketches of (b) Generating k = 1024 length
length k for data examples sketches for data examples
with n = 1, 000 objects
with n objects

Fig. 8: (Task 2) Average running time of Stream-FastGM and
Lemiesz’s sketch on synthetic data.

Results on the simulated scenario. Following the experimental setting in [8], we conduct experiments on simulated
multi-hop wireless sensor networks where sensors use a
braid chain strategy to guarantee the robustness of communication. In Fig. 9, we show the topology of simulated
networks. A braided chain consists of two sequences of
A
B
B
B
nodes (sensors) SA = [sA
1 , · · · , sd ] and S = [s1 , · · · , sd ].
Nodes with the same position in the sequences, such as sA
1
and sB1 , are considered as nodes in the same layer. Because
the transfer path (edges in the network topology) is unstable.
To guarantee the transfer of traffic packets, the node in
the previous layer redundantly transfers traffic packets to
all nodes in the next layer. Specifically, the transfer path
between nodes in the same sensors sequence and between
different sensors sequences (e.g., the edge between sA
1 and
A
B
sA
2 , the edge between s1 and s2 ) work well in chance p1 and
p2 , respectively. For example, a traffic packet in node sA
1 is

1

sB1 . For the traffic packet sequence that passes through each
node in the network, we build a sketch for it and use the
sketch to estimate the total size of distinct packets appearing
in this sequence. In this case, the weighted cardinality of the
sequence represents the sum of distinct packets’ sizes in the
sequence. The reasons to build a sketch rather than simply
use a counter are: 1) the traffic packet sequences passing
through nodes in layers behind the second layer contain
repetitive traffic packets, which causes the double-counting
problem; 2) aggregating sketches rather than packets will not
cause an explosion of packets even when the network follows
a flooding strategy while guaranteeing the certain accuracy
of network communication [8]. More than that, based on the
sketches we are able to obtain more useful information about
the network and we conduct the following experiments to
demonstrate this. Ground truth results are shown as solid
lines, and estimations obtained from sketches are shown as
dashed lines. For clarity, we use the symbol Ns to represent the
weighted set of packets that occurred in traffic packet sequence Πs
of a node rather than NΠs .
In Fig. 10a, black and orange lines represent the size
B
of packets from source sA
1 and s1 respectively. The size of
A
distinct packets
P received by a node s` , 1 ≤ ` ≤ d is defined
as |NsA |w = i∈N sA vi , where i represents a traffic packet
`
`
A
and vi is the size of packet i. For a node sA
` in sequence S ,
B
the sizes of distinct packets sent from source sA
and
source
s
1
1
are computed as |NsA ∩NsA |w and |NsB ∩NsA |w , respectively.
1
`
1
`
In Fig. 10b, we show the results of estimating the average

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

8,000

Lemiesz′ s sketch
FastGM

6,000

From source B

4,000

1
Mean size

0.8

2,000
0

Lemiesz′ s sketch
FastGM

1.2

0.6
0

5

From source A
10 15 20 25 30
layer

0.4

0

5

10 15 20 25 30
layer

(a) The total size of distinct
packets from sources A and B
at node sA
` in each layer.

(b) The mean size of distinct packets passing through
node sA
` in each layer.

5,000

0.8

Lemiesz′ s sketch
FastGM

4,000

Lemiesz′ s sketch
FastGM

0.6

3,000
Total size of
lost packets

2,000
1,000
0

0

5

0.4

weighted Jaccard
Similarity

0.2
0

10 15 20 25 30

0

5

10 15 20 25 30

layer

(c) The total size of lost packets
sent from source sA
1 in each
layer.

layer

(d) The weighted Jaccard
similarity between two
nodes in each layer.

Fig. 10: (Task 2) In Fig. 10a-10d solid lines represent ground
truth results, red circle and blue cross points respectively
represent the estimation results based on Lemiesz’s sketches
and FastGM sketches with length k = 200. All results are
obtained from a simulated sensor network with d = 30 layers
where each data source generates n = 10, 000 packets.

101

Lemiesz s sketch
Stream-FastGM

100

101

′

Lemiesz s sketch
Stream-FastGM

100

10−1

10−1
10−2 6
2

102

′

Time (second)

Time (second)

102

27

28

29 210 211 212
k

(a) Generating sketches of different lengths k on each node
of simulated sensor networks
with d = 30 layers.

10−2
20 25 30 35 40 45 50
layers

(b) Generating sketches of
length k = 1024 for nodes
of simulated networks with
different depth of layers.

Fig. 11: (Task 2) Average running time of Stream-FastGM
and Lemiesz’s sketch on simulated sensor networks.

size of distinct packets on each node in sensor sequence SA .
In Fig. 10c, we use the sketches to estimate the total size of
lost packets from source sA
1 in each layer of the braided chain.
The set of lost packets from source sA
1 in a layer NLA can be
`
obtained from NLA = NsA \ (NsA ∪ NsB ), where NsA ∪ NsB
`
1
`
`
`
`
denotes the set of the distinct packets passing through at least
B
one of nodes sA
` and s` , and set NsA \ (NsA ∪ NsB ) represents
1

`

`

the set of packets generated by source sA
1 but not received by
B
node sA
` or node s` . Note that each node in a layer receives a

11

mixture of some packets in traffic packets from both sources
B
sA
1 and s1 . Therefore, we can use the weighted Jaccard
similarity JW between traffic packets sets NsA and NsB , i.e.,
|N A ∩N B |w

`

`

s
s
JW (NsA , NsB ) = |N ` ∪N ` |w , to measure the proportion of
A
l
`
s`
sB
`

the total size of identical packets passing through the two
nodes. We show the results in Fig. 10d. Given the GumbelMax sketches of two arbitrary sets NA and NB , Lemiesz [8]
proposed a series of methods to estimate the weighted
cardinality of both union and intersection |NA ∪ NB |w and
|NA ∩ NB |w , the weighted Jaccard similarity JW (NA , NB ),
the weighted cardinality of relative complement |NA \ NB |w
from these sketches, and these methods can be extended to
multiple sets. In our experiments, we use the same methods
to compute the total size of packets from sources A and B
at each sensor node, the total size of lost packets at each
sensor node, and the weighted Jaccard similarity between
two nodes in each layer. As we analyzed above, the ~
y part of
FastGM sketch is the same as Lemiesz’s sketch, so they have
the same performance in each experiment.
To demonstrate the efficiency of our Stream-FastGM, in
Fig. 11a we report the running time of generating sketches
with different lengths k . When k = 2048, our Stream-FastGM
is 52 times faster than Lemiesz’s sketch, and the results
show that our Stream-FastGM gets faster than Lemiesz’s
sketch when k gets larger. We also conduct experiments on
simulated sensor networks with different depths of layers,
as shown in Fig. 11b, our Stream-FastGM is 47 times faster
than Lemiesz’s sketch on average.

5

R ELATED W ORK

5.1

Jaccard Similarity Estimation

Broder et al. [14] proposed the first sketch method MinHash
to compute the Jaccard similarity of two sets (or binary
vectors). MinHash builds a sketch consisting of k registers
for each set. Each register uses a hash function to keep
track of the set’s element with the minimal hash value. To
further improve the performance of MinHash, [31], [12],
[32] developed several memory-efficient methods. Li et
al. [33] proposed One Permutation Hash (OPH) to reduce
the time complexity of processing each element from O(k)
to O(1) but this method may exhibit large estimation errors
because of the empty buckets. To solve this problem, several
densification methods [17], [18], [19], [34] were developed to
set the registers of empty buckets according to the values of
non-empty buckets’ registers.
Besides binary vectors, a variety of methods have also
been developed to estimate generalized Jaccard similarity on
weighted vectors. For vectors consisting of only nonnegative
integer weights, Haveliwala et al. [35] proposed to add
a corresponding number of replications of each element
in order to apply the conventional MinHash. To handle
more general real weights, Haeupler et al. [36] proposed to
generate another additional replication with a probability
that equals the floating part of an element’s weight. These
two algorithms are computationally intensive when computing hash values of massive replications for elements with
large weights. To solve this problem, [37], [38] proposed to
compute hash values only for a few necessary replications

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

(i.e., "active indices"). ICWS [39] and its variations such as
0-bit CWS [40], CCWS [41], PCWS [42], I2 CWS [43] were
proposed to improve the performance of CWS [38]. The CWS
algorithm and its variants all have the time complexity of
O(n+ k), where n+ is the number of elements with positive
weights. Recently, Otmar [30] proposed another efficient algorithm BagMinHash for handling high-dimensional vectors.
BagMinHash is faster than ICWS when the vector has a large
number of positive elements, e.g., n+ > 1, 000, which may
not hold for many real-world datasets. The above methods
all estimate the weighted Jaccard similarity. Ryan et al. [6]
proposed a Gumbel-Max Trick based sketching method, P MinHash, to estimate another novel Jaccard similarity metric,
probability Jaccard similarity JP . They also demonstrated
that the proposed probability Jaccard similarity JP is scaleinvariant and more sensitive to changes in vectors. However,
the time complexity of P -MinHash processing a weighted
vector is O(n+ k), which is infeasible for high-dimensional
vectors.
5.2

Cardinality Estimation

The regular problem of cardinality estimation aims to compute the number of distinct elements in the set of interest,
which is typically given as a sequence containing duplicated
elements [44]. To address this problem, a number of sketch
methods such as LPC [45], LogLog [46], HyperLogLog [47],
RoughEstimator [48], HLL-TailCut+ [49], and HLL++ [50]
build a sketch consisting of m bits/counters for a set. The
sketch is small (e.g., m = 1, 000) and can be efficiently
updated, which handles each element with few operations.
The generated sketch is finally used to estimate the set’s
cardinality. In addition, [51], [52] exploit martingale estimation and maximum likelihood estimation to improve
the estimation accuracy of the above methods. For some
applications, there may exist many sets of which sizes vary
significantly. To reduce the memory cost of building a sketch
for each set, a number of works [53], [54], [55], [56], [57], [58],
[59] propose to implement m independent hash functions to
randomly map each sketch into a large shared bit/counter
array, where each sketch can be rebuilt by randomly sampling
m bits/counters from the shared array.
Recently, [60], [8] generalized the problem of cardinality
estimation to a weighted version, where each element is
associated with a fixed positive weight. The goal of weighted
cardinality estimation is to estimate the total sum of weights
for all distinct elements in the stream of interest. The
drawback of the sketch methods in [60], [8] is their high
computational costs.

6

C ONCLUSION

In this paper, we develop an efficient algorithm FastGM
to compute a non-negative vector’s k -length Gumbel-Max
sketch. We propose a novel model, Queuing model with k servers and n-queues, to model the procedure of computing
the Gumbel-Max sketch in a brief and practical way. Based
on the proposed model, we optimize the procedure of
ln a
generating k random variables − vii,j of an element vi
in a vector. We theoretically prove that our FastGM reduces
the time complexity of generating a k -length Gumbel-Max

12

sketch from O(n+ k) to O(k ln k + n+ ), where n+ is the
number of the vector’s positive elements. We conduct two
tasks probability Jaccard similarity estimation and weighted
cardinality estimation to demonstrate the efficiency and
effectiveness of FastGM. Experimental results show that
our FastGM is around 10 times faster than state-of-the-art
methods, without losing any estimation accuracy.

ACKNOWLEDGMENTS
The authors would like to thank the anonymous reviewers for
their comments and suggestions. This work was supported
in part by National Natural Science Foundation of China
(U22B2019, 62272372, 61902305), MoE-CMCC "Artificial Intelligence" Project (MCM20190701).

R EFERENCES
[1]

Y. Qi, P. Wang, Y. Zhang, J. Zhao, G. Tian, and X. Guan, “Fast
generating A large number of gumbel-max variables,” in WWW,
2020, pp. 796–807.
[2] R. D. Luce, Individual choice behavior: A theoretical analysis. Courier
Corporation, 1959.
[3] D. Yang, B. Li, and P. Cudré-Mauroux, “Poisketch: Semantic place
labeling over user activity streams,” Université de Fribourg, Tech.
Rep., 2016.
[4] D. Yang, B. Li, L. Rettig, and P. Cudré-Mauroux, “Histosketch:
Fast similarity-preserving sketching of streaming histograms with
concept drift,” in IEEE ICDM. IEEE, 2017, pp. 545–554.
[5] ——, “D2 histosketch: discriminative and dynamic similaritypreserving sketching of streaming histograms,” IEEE TKDE, pp.
1–1, 2018.
[6] R. Moulton and Y. Jiang, “Maximally consistent sampling and
the jaccard index of probability distributions,” arXiv preprint
arXiv:1809.04052, 2018.
[7] D. Yang, P. Rosso, B. Li, and P. Cudre-Mauroux, “Nodesketch:
Highly-efficient graph embeddings via recursive sketching,” in
SIGKDD, 2019.
[8] J. Lemiesz, “On the algebra of data sketches,” Proc. VLDB Endow.,
vol. 14, no. 9, pp. 1655–1667, may 2021.
[9] M. Henzinger, “Finding near-duplicate web pages: a large-scale
evaluation of algorithms,” in SIGIR. ACM, 2006, pp. 284–291.
[10] G. S. Manku, A. Jain, and A. Das Sarma, “Detecting near-duplicates
for web crawling,” in WWW. ACM, 2007, pp. 141–150.
[11] Y. Bachrach, E. Porat, and J. S. Rosenschein, “Sketching techniques
for collaborative filtering,” in IJCAI, 2009.
[12] M. Mitzenmacher, R. Pagh, and N. Pham, “Efficient estimation for
high similarities using odd sketches,” in WWW, 2014, pp. 109–118.
[13] A. Gionis, P. Indyk, and R. Motwani, “Similarity search in high
dimensions via hashing,” in PVLDB, 1999, pp. 518–529.
[14] A. Z. Broder, M. Charikar, A. M. Frieze, and M. Mitzenmacher,
“Min-wise independent permutations,” J. Comput. Syst. Sci., vol. 60,
no. 3, pp. 630–659, Jun. 2000.
[15] M. S. Charikar, “Similarity estimation techniques from rounding
algorithms,” in STOC, 2002, pp. 380–388.
[16] E. Buchnik, E. Cohen, A. Hasidim, and Y. Matias, “Self-similar
epochs: Value in arrangement,” in ICML, 2019, pp. 841–850.
[17] A. Shrivastava and P. Li, “Improved densification of one permutation hashing,” in UAI, 2014, pp. 732–741.
[18] ——, “Densifying one permutation hashing via rotation for fast
near neighbor search,” in ICML, 2014, pp. 557–565.
[19] A. Shrivastava, “Optimal densification for fast and accurate
minwise hashing,” in ICML, 2017, pp. 3154–3163.
[20] R. Motwani and P. Raghavan, “3.6 the coupon collector’s problem,
randomized algorithms,” 1995.
[21] A. Rényi, “On the theory of order statistics,” Acta Mathematica
Hungarica, 1953.
[22] R. A. Fisher and F. Yates, Statistical tables for biological, agricultural
and medical research. Hafner Publishing Company, 1953.
[23] “Variance of the maximum of n independent exponentials,”
https://math.stackexchange.com/questions/3175307/
variance-of-the-maximum-of-n-independent-exponentials.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

[24] W. Wei, B. Li, C. Ling, and C. Zhang, “Consistent weighted
sampling made more practical,” in WWW, 2017, pp. 1035–1043.
[25] D. D. Lewis, Y. Yang, T. G. Rose, and L. Fan, “Rcv1: A new
benchmark collection for text categorization research,” JMLR, vol. 5,
no. 2, pp. 361–397, 2004.
[26] S. S. Keerthi and D. DeCoste, “A modified finite newton method for
fast solution of large scale linear svms,” Journal of Machine Learning
Research, no. 6, pp. 341–361, 2005.
[27] “Libimseti.cz network dataset – KONECT,” Apr. 2017. [Online].
Available: http://konect.uni-koblenz.de/networks/libimseti
[28] A. Zubiaga, “Enhancing navigation on wikipedia with social tags,”
in Wikimania, 2009.
[29] “Movielens 10m network dataset – KONECT,” Apr. 2017.
[Online]. Available: http://konect.uni-koblenz.de/networks/
movielens-10m_rating
[30] O. Ertl, “Bagminhash-minwise hashing algorithm for weighted
sets,” in SIGKDD. ACM, 2018, pp. 1368–1377.
[31] P. Li and A. C. König, “b-bit minwise hashing,” in WWW, 2010, pp.
671–680.
[32] P. Wang, Y. Qi, Y. Zhang, Q. Zhai, C. Wang, J. C. S. Lui, and X. Guan,
“A memory-efficient sketch method for estimating high similarities
in streaming sets,” in SIGKDD, 2019, pp. 25–33.
[33] P. Li, A. B. Owen, and C. Zhang, “One permutation hashing,” in
NIPS, 2012, pp. 3122–3130.
[34] S. Dahlgaard, M. B. T. Knudsen, and M. Thorup, “Fast similarity
sketching,” in FOCS. IEEE, 2017, pp. 663–671.
[35] T. Haveliwala, A. Gionis, and P. Indyk, “Scalable techniques for
clustering the web,” 2000.
[36] B. Haeupler, M. Manasse, and K. Talwar, “Consistent weighted
sampling made fast, small, and easy,” arXiv preprint arXiv:1410.4266,
2014.
[37] S. Gollapudi and R. Panigrahy, “Exploiting asymmetry in hierarchical topic extraction,” in CIKM. ACM, 2006, pp. 475–482.
[38] M. Manasse, F. McSherry, and K. Talwar, “Consistent weighted
sampling,” Tech. Rep., June 2010.
[39] S. Ioffe, “Improved consistent sampling, weighted minhash and L1
sketching,” in ICDM, 2010, pp. 246–255.
[40] P. Li, “0-bit consistent weighted sampling,” in SIGKDD, 2015, pp.
665–674.
[41] W. Wu, B. Li, L. Chen, and C. Zhang, “Canonical consistent
weighted sampling for real-value weighted min-hash,” in ICDM,
2016, pp. 1287–1292.
[42] ——, “Consistent weighted sampling made more practical,” in
WWW, 2017, pp. 1035–1043.
[43] W. Wu, B. Li, L. Chen, C. Zhang, and P. Yu, “Improved consistent
weighted sampling revisited,” IEEE TKDE, 2018.
[44] H. Lan, Z. Bao, and Y. Peng, “A survey on advancing the dbms
query optimizer: Cardinality estimation, cost model, and plan
enumeration,” Data Science and Engineering, vol. 6, no. 1, pp. 86–101,
2021.
[45] K.-Y. Whang, B. T. Vander-Zanden, and H. M. Taylor, “A linear-time
probabilistic counting algorithm for database applications,” TODS,
vol. 15, no. 2, pp. 208–229, 1990.
[46] M. Durand and P. Flajolet, “Loglog counting of large cardinalities,”
in ESA, 2003, pp. 605–617.
[47] P. Flajolet, É. Fusy, O. Gandouet, and F. Meunier, “Hyperloglog:
the analysis of a near-optimal cardinality estimation algorithm,” in
DMTCS, 2007, pp. 137–156.
[48] D. M. Kane, J. Nelson, and D. P. Woodruff, “An optimal algorithm
for the distinct elements problem,” in PODS, 2010, pp. 41–52.
[49] Q. Xiao, Y. Zhou, and S. Chen, “Better with fewer bits: Improving
the performance of cardinality estimation of large data streams,” in
INFOCOM, 2017, pp. 1–9.
[50] S. Heule, M. Nunkesser, and A. Hall, “Hyperloglog in practice:
Algorithmic engineering of a state of the art cardinality estimation
algorithm,” in EDBT, 2013, pp. 683–692.
[51] D. Ting, “Streamed approximate counting of distinct elements:
Beating optimal batch methods,” in SIGKDD, 2014, pp. 442–451.
[52] O. Ertl, “New cardinality estimation algorithms for hyperloglog
sketches,” arXiv preprint arXiv:1702.01284, 2017.
[53] Q. Zhao, A. Kumar, and J. J. Xu, “Joint data streaming and sampling
techniques for detection of super sources and destinations,” in IMC,
2005, pp. 77–90.
[54] M. Yoon, T. Li, S. Chen, and J.-K. Peir, “Fit a spread estimator in
small memory,” in INFOCOM, 2009, pp. 504–512.

13

[55] P. Wang, X. Guan, T. Qin, and Q. Huang, “A data streaming method
for monitoring host connection degrees of high-speed links,” TIFS,
vol. 6, no. 3, pp. 1086–1098, 2011.
[56] Q. Xiao, S. Chen, M. Chen, and Y. Ling, “Hyper-compact virtual
estimators for big network data based on register sharing,” in
SIGMETRICS, 2015, pp. 417–428.
[57] P. Wang, P. Jia, X. Zhang, J. Tao, X. Guan, and D. Towsley, “Utilizing
dynamic properties of sharing bits and registers to estimate user
cardinalities over time,” in ICDE, 2019, pp. 1094–1105.
[58] P. Jia, P. Wang, Y. Zhang, X. Zhang, J. Tao, J. Ding, X. Guan, and
D. Towsley, “Accurately estimating user cardinalities and detecting
super spreaders over time,” TKDE, vol. 34, no. 1, pp. 92–106, 2020.
[59] D. Ting, “Approximate distinct counts for billions of datasets,” in
SIGMOD, 2019, pp. 69–86.
[60] R. Cohen, L. Katzir, and A. Yehezkel, “A unified scheme for
generalizing cardinality estimators to sum aggregation,” Inf. Process.
Lett., vol. 115, no. 2, pp. 336–342, 2015.

Yuanming Zhang received a B.S. degree in automation from Chongqing University, Chongqing,
China, in 2017. He is currently working toward a
graduate degree at the MOE Key Laboratory for
Intelligent Networks and Network Security, Xi’an
Jiaotong University, Xi’an, China. His research
interests include anomaly detection, encrypted
traffic analysis, and Internet traffic measurement
and modeling.

Pinghui Wang (Senior Member, IEEE) is currently a Professor with the MOE Key Laboratory
for Intelligent Networks and Network Security,
Xi’an Jiaotong University, Xi’an, China, and also
with the Shenzhen Research Institute, Xi’an Jiaotong University, Shenzhen, China. His research
interests include internet traffic measurement and
modeling, traffic classification, abnormal detection, and online social network measurement

Yiyan Qi received a B.S. in automation engineering and a Ph.D. degree in automatic control
from Xi’an Jiaotong University, Xi’an, China, in
2014 and 2021 respectively. He is currently a
Researcher at the International Digital Economy
Academy (IDEA). Prior to joining IDEA, he was
working at Tencent. His current research interests
include abnormal detection, graph mining and
embedding, and recommender systems.

Kuankuan Cheng is currently working toward an
undergraduate degree at Xi’an Jiaotong University, Xi’an, China. His research interests include
streaming data processing and encrypted traffic
analysis.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

Junzhou Zhao received B.S. (2008) and Ph.D.
(2015) degrees in control science and engineering from Xi’an Jiaotong University. He is currently
an associate professor at the School of Cyber Science and Engineering, Xi’an Jiaotong University.
His research interests include graph data mining
and streaming data processing.

Guangjian Tian received a Ph.D. degree in
computer science and technology from Northwestern Polytechnical University, Xi’an, China, in
2006. He is currently a principal researcher in
Huawei Noah’s Ark Lab. Before that, he was a
postdoctoral research fellow with the Department
of Electronic and Information Engineering, The
Hong Kong Polytechnic University, Hong Kong.
His research interests include temporal data
analysis, deep learning, and data mining with
a specific focus on different industry applications.

Xiaohong Guan (Fellow, IEEE) received a Ph.D.
degree in electrical engineering from the University of Connecticut, Storrs, in 1993. Since 1995,
he has been with the Department of Automation,
Tsinghua National Laboratory for Information
Science and Technology, and the Center for
Intelligent and Networked Systems, Tsinghua
University. He is currently with the MOE Key
Laboratory for Intelligent Networks and Network
Security, Faculty of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China,
where he is also the Dean of the Faculty of Electronic and Information
Engineering. He is an Academician of the Chinese Academy of Sciences.

14

